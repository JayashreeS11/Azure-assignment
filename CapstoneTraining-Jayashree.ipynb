{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70277e2-5903-4852-87eb-528549bf3f56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4006ac5c-cbf5-43e8-8193-13b3ccc67218",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28e457c6-fa22-46f2-9894-34f5b894f585",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data_path = dbutils.widgets.get(\"train_data\")\n",
    "# here instead of directly giving path, take from adf parameters\n",
    "\n",
    "# Load train data from Parquet\n",
    "train_data = spark.read.parquet(train_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec2ee5d-49f9-4d88-b529-b62dddc8eb08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7478efe-19d8-434b-8f3f-861b68c09779",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-7.003280721090691e-14,-6.427689675616689e-15,5.842492157733723e-15,-2.1182478323304876e-14,-8.198562512768448e-15,7.064576272058961e-15,-9.495068460411978e-15,6.503040566376214e-14,-4.027475827968103e-14,-3.6911352113981356e-15,4.6042390481824355e-14,0.8075694397346824]\nIntercept: 5.64\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"quality\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "print(\"Coefficients: {}\".format(lr_model.coefficients))\n",
    "print(\"Intercept: {:.2f}\".format(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "779bc2ca-e6b1-4f6d-8aea-483d238858d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"/mnt/capstonejs/models/lr_model\"\n",
    "lr_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f6e1150-190e-443b-a039-bcb7272a550d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f95798-8c21-4ab1-8362-399f3fa3afc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol=\"scaled_features\", labelCol=\"quality\",maxDepth=5)\n",
    "dt_model = dt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c47bf4-05a6-4643-971d-f22d4ac1871d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt_model.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(dt_model.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"quality\")\n",
    "\n",
    "# Define CrossValidator with 5 folds\n",
    "cv = CrossValidator(estimator=dt,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47290ec-2424-449f-85dc-a60b45be9c6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_model_dt = cv.fit(train_data)\n",
    "dt_best = cv_model_dt.bestModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed0fd498-cf80-4252-956b-422cb2dfdc49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"/mnt/capstonejs/models/dt_model\"\n",
    "dt_model.write().overwrite().save(model_path)\n",
    "\n",
    "model_path = \"/mnt/capstonejs/models/dt_best\"\n",
    "dt_best.write().overwrite().save(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bc7c04b-71b4-4d40-9ed4-1228eafc8ac6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499ddb6b-3684-4612-8cc9-9dea8b9fd861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_classes = train_data.select(\"quality\").distinct().count()\n",
    "rf = RandomForestClassifier(labelCol=\"quality\", featuresCol=\"scaled_features\")\n",
    "\n",
    "# Define parameter grid for grid search\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"quality\")\n",
    "\n",
    "# Define CrossValidator with 5 folds\n",
    "cv = CrossValidator(estimator=rf,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5)\n",
    "\n",
    "# Fit CrossValidator\n",
    "cv_model_rf = cv.fit(train_data)\n",
    "\n",
    "# Get best model from CrossValidator\n",
    "rf_model = cv_model_rf.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8275118d-aa6a-4194-8844-de19182fe86e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"/mnt/capstonejs/models/rf_model\"\n",
    "rf_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2e70b9-57de-4519-aab7-5aafd719a7d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CapstoneTraining-Jayashree",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
